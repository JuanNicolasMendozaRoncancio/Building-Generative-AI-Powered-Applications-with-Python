{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45499be",
   "metadata": {},
   "source": [
    "# Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001f9d0",
   "metadata": {},
   "source": [
    "## Generative AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030732f4",
   "metadata": {},
   "source": [
    "In the world of generative AI, these four models have made a significant impact. Variational autoencoders, generative adversarial networks, transformer-based models, and diffusion models. Each model employs a different type of deep learning architecture and applies probabilistic techniques. Let's gain insight into how they work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1efe8a",
   "metadata": {},
   "source": [
    "### Variational autoencoders\n",
    "\n",
    "Variational autoencoders or VAEs are the most popular of all generative AI models for two reasons. \n",
    "-   They work with a diverse range of training data, such as images, text, and audio. \n",
    "- They rapidly reduce the dimensionality of your image, text, or audio to create a newer improved version.\n",
    "\n",
    "\n",
    "First, the encoder, which is a self-sufficient neural network, studies the probability distribution of the input data. In simple terms, this means that it isolates the most useful data variables. This allows the encoder to create a compressed representation of the data sample and store it in the latent space. You can think of this latent space as a mathematical space within the model's architecture, where large dimensional data is represented in a compressed format. Next, the decoder or reverse encoder, which is also a self-sufficient neural network, decompresses the compressed representation in the latent space to generate the desired output. Basically, the algorithms are trained using a maximum likelihood principle, which means they try to minimize the difference between the original input data and the reconstructed output. \n",
    "\n",
    "Although VAEs are trained in a static environment, their latent space is characterized as continuous. Therefore, they can generate new samples by randomly sampling from the probability distribution of data. Because they can produce realistic and varied images with little training data, VAEs are used in image synthesis, data compression, and anomaly detection tasks. For example, the entertainment industry uses VAEs to create game maps and animate avatars. The finance industry uses VAEs to forecast the volatility surfaces of stocks. The healthcare sector uses VAEs to detect diseases using electrocardiogram signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474b542",
   "metadata": {},
   "source": [
    "### GANs\n",
    "\n",
    "Generative adversarial network organ is another type of generative AI model that uses imagery and textual input data. In this model, two convolutional neural networks or CNNs, compete with each other in an adversarial game. One CNN plays the role of a generator and is trained on a vast dataset to produce data samples. The other CNN plays the role of a discriminator and tries to distinguish between real and fake samples. Based on the discriminator's responses, the generator seeks to produce more realistic data samples. GANs can generate new realistic-looking images, perform a style transfer or image to image translation and even create deep fakes. The finance industry uses GANs to train models for loan pricing or generating time series. \n",
    "Tools such as SpaceGAN work with geospatial data and videos StyleGAN2 is known for creating video game characters. Unlike variational autoencoders, GANs can be challenging to train as they require a large amount of data and heavy computational power. They can potentially create false material which is an ethical concern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d025fbf",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Transformer-based models were introduced a few years ago when recurrent neural networks or RNN started facing a problem called vanishing gradients. Due to this problem, RNNs were struggling to process long sequences of text. To get around this challenge, transformers were built with attention mechanisms that could focus on the most valuable parts of the text while filtering out the unnecessary elements. This allowed transformers to model long-term dependencies in text. \n",
    "For instance, when you enter a simple prompt, the two-stack transformer architecture uses an encoder-decoder mechanism to generate coherent and contextually relevant text. As transformer models can query extensive databases, they are able to create large language models and perform natural language processing tasks such as picture creation, music synthesis, and even video synthesis. This marks a significant breakthrough in our approach to content creation and offers many opportunities for innovation as has been seen with GPT 3.5 and its subsequent versions, BERT and T5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b0774",
   "metadata": {},
   "source": [
    "### Difusion models\n",
    "\n",
    "Diffusion models are a more recent addition to the world of generative AI models. They address the systematic decay of data that occurs due to noise in the latent space. By applying the principles of diffusion, these models try to prevent information loss. Just as in the diffusion process, where molecules move from high-density to low-density areas, diffusion models move noise to and from a data sample using a two-step process. \n",
    "Step 1 is forward diffusion, in which algorithms gradually add random noise to training data. Step 2 is reverse diffusion, in which algorithms turn the noise around to recover the data and generate the desired output. Open AI's Dall-E2, Stability AI's Stable Diffusion XL, and Google's Imagen are mature diffusion models that generate high-quality graphical content. Similar to variational autoencoders, diffusion models also try to optimize data by first projecting it onto the latent space and then recovering it back to the initial state. However, a diffusion model is trained using a dynamic flow and therefore takes longer to train. Then why are these models considered the best option for creating generative AI models? Because they train hundreds, maybe even an unlimited number of layers and have shown remarkable results in image synthesis and video generation. \n",
    "Experiments with generative AI models continue unabated as unsupervised algorithms throw up one surprise after another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb52db",
   "metadata": {},
   "source": [
    "## Fundation models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523b452",
   "metadata": {},
   "source": [
    "Stanford University Center for Research on Foundation Models defines a foundation model as a new successful paradigm for building AI systems. Train one model on a huge amount of data and adapt it to many applications. We call such a model a foundation model. Let's explore this definition more closely. The first part of this definition says train one model on a huge amount of data. How does this work? \n",
    "A foundation model is a large general purpose self supervised model that is pre-trained on vast amounts of unlabeled data, establishing billions of parameters. Pre-training is a technique during which unsupervised algorithms are repeatedly given the liberty to make connections between diverse pieces of information. This allows foundation models to develop multimodal, multi-domain capabilities. Such that they can accept input prompts and multiple modalities such as text, image, audio, or video formats and perform complex and creative tasks, such as answering questions, summarizing documents, writing essays, solving equations, extracting information from images, even developing code. This broad skill set makes these models relevant to multiple domains. This is in contrast the smaller generative AI models, which are trained on restricted domain data and requested to perform limited tasks. For instance, OpenAI's Dall-E family of models are considered foundation models because they can perform many image related tasks. \n",
    "In contrast, AlexNet is not classified as a foundational model as it only performs image classification tasks. Therefore, we can clarify that while all foundation models have generative AI capabilities, not all generative AI models are foundation models. When foundation models are trained on vast natural language processing databases, they are called Large Language Models or LLMs. LLMs develop independent reasoning allowing them to respond to queries uniquely, for example, OpenAI's, GPT and class of models including GPT-3, which is pre-trained on 175 plus billion parameters and GPT-4, which is pre-trained on an estimated 180 plus trillion parameters. Other examples of large language models include Google's Pathway Language Model pre-trained on 540 billion parameters, Meta's Large Language Model Meta AI pre trained on 65 billion parameters. Google's Bert pre-trained on 340 million plus parameters. Meta's Galactica, an LLM for scientists pre-trained on 48 million papers, lectures, textbooks, and websites. \n",
    "Technology innovation institutes Falcon 7B pre-trained on 1.5 trillion tokens and Microsoft's Orca pre-trained at 13 billion parameters and small enough to run off a laptop. It's likely that these parameters may change as generative AI tools evolve in their scope and size. Another aspect of models evolving is their ability to adapt. The definition also suggests that we can adapt a foundation model to many applications. This is possible because of the broad based training of foundation models, which allows them to learn new things and adapt to new situations. Small businesses can leverage this capability to create customized, more efficient, generative AI models at an affordable cost. This is why foundation models are also called base models. \n",
    "They help make AI systems more accessible to businesses and individuals who do not have the resources to train their models from scratch. In this way, foundation models enable enterprises to shrink time to value from months to weeks. Take for example the evolution of chatbots. OpenAI's GPT-3 and GPT-4 are foundation models that power the ChatGPT chatbot. Google's PaLM powers the Google barred chatbot. These are today's unreasonably clever chatbots. However, if we think back to how early chatbots functioned, we realize that they were trained on smaller datasets which confine their generative capabilities. \n",
    "While they could predict responses based on keywords, they could only provide a predetermined response. In contrast, chatbots today are pre-trained multiple times on extensive datasets. They are therefore able to increase their word prediction accuracy and respond in a more helpful and creative manner. Try this will you? If you type a single sentence prompt and chatGPT, you'll likely get more than a basic response depending on what your prompt requested. The chatbot may write a comparative essay, create an infographic, design a checklist or script a short story. OpenAI's GPT-3 is also the foundation model for Dall-E. An image generation tool that responds to text prompts. \n",
    "For a single text prompt, Dall-E generates four high resolution images in multiple styles, including photo realistic images and paintings. Another clarification to note here, while all large language models are foundation models, not all foundation models are large language models. Some foundation models use diffusion architecture capabilities to improve the scale and scope of their image generation capabilities. For instance, Dall-E uses transformer architecture. But the latest version of Dall-E uses sound diffusion to generate images from text. Stability, AI stable diffusion uses diffusion architecture to generate high resolution images in realistic cartooning and abstract styles based on the user's description. Google's imaging uses a cascaded diffusion model built on an LLM to generate images from text prompts. \n",
    "As foundation models evolve in their strengths and applications, we have seen some limitations. Firstly, the desired output may be biased if the data on which the foundation model is trained is biased. Secondly, LLMs can hallucinate responses. That means they generate false information because they misinterpret the context of data parameters within a dataset. Therefore, you must verify the accuracy of the output produced by a generative AI chatbot. With a little caution, you can enjoy the many benefits foundation models offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618ea61",
   "metadata": {},
   "source": [
    "## Project 1:Image Captioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e58ca",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "\n",
    "Hugging Face is an open source artificial intelligence platform where scientists, developers and businesses collaborate to build personalized machine learning tools. The platform was built with the purpose of creating a hub for the open source AI community to share models, data sets and applications. This way AI becomes accessible to all types of users, even those who do not have the budget or bandwidth to build machine learning applications independently. Hugging Face is therefore credited with democratizing AI as everyone comes together to benefit from smaller, curated models. Challenging the general one model to rule assumption. \n",
    "At first the hugging phase community focused on creating transformer-based models to leverage the capabilities of natural language processing, or NLP. However, today the platform offers various machine learning tools for generating text, images, audio, and video. Currently, the Hugging Face platform hosts over 250,000 open models, 50,000 data sets, and one million open demos. This list keeps growing. Scientists and developers use Hugging Face to build, train and deploy their AI models. They have access to the platform's open source transformer library, which has over 25,000 pretrained models for PyTorch, Tensorflow, and Google Jax. PyTorch is a deep learning library. \n",
    "Tensorflow is a machine learning platform and Google Jax is a machine learning framework. The models in the library perform varied tasks, such as text generation, question answering, summarization, automatic speech recognition, and image segmentation, just to name a few. Users can filter these models by name to find an existing model or share their own model with the library. Developers can also host demos of generative AI applications on the Spaces tab, allowing users to interact and validate them. How do businesses benefit from the platform? Hugging Face offers businesses the enterprise hub from where they can access pre trained models and data sets. This allows businesses to leverage existing infrastructure rather than build models from scratch. \n",
    "Not only does this reduce their carbon footprint, time, and cost to scale, it also allows businesses to train the models on proprietary data and relevant use cases. Additionally, Hugging Face helps businesses A, add or remove features to improve the efficiency of their models. B, evaluate their generative AI models to filter biased data. C, create multimodal applications with text, image, audio, and video generation capabilities. More than 50,000 large and small companies actively use Hugging Face. For example, Writer, a generative AI solution provider, hosts its Palmera Large Language Models or LLMs on Hugging Face. Intel has officially joined Hugging Face Hardware Partner program and is collaborating with Hugging Face to build state- of-the-art machine learning hardware and end-to-end machine learning workflows. \n",
    "Even universities and non-profits are part of Hugging Face. Among other services. Hugging Face offers an expert acceleration program to guide non-developers on machine learning models. HuggingChat is the first open source alternative to ChatGPT. To protect its users, Hugging Face complies with service organization control type 2 regulation. This means ensuring user data security, availability, processing integrity, confidentiality, and privacy. Taking its collaborative efforts one step further, Hugging Face has entered into a unique partnership with watsonx.ai, IBM's next generation enterprise studio for AI builders. \n",
    "Watsonx.ai offers select Hugging Face models in its studio to help its community of builders train, test and deploy all types of machine learning and generative AI applications. This way, the studio leverages the diversity of data community strength and open source libraries that Hugging Face provides. On its end, Hugging Face creates open source versions of IBM's LLMs and makes them available on a hub. Both entities believe in open source technology, both bet on the community to create value in the AI space. As proprietary AI models can quickly become obsolete, Hugging Face may develop an edge over the big five in AI, namely, Google, Open AI, Meta, IBM, and Microsoft. This is because it supports and is supported by the open source AI community that keeps innovating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ace6a6",
   "metadata": {},
   "source": [
    "### BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf6331",
   "metadata": {},
   "source": [
    "#### Introduction to Hugging Face Transformers\n",
    "\n",
    "Hugging Face Transformers is a popular open-source library that provides state-of-the-art natural language processing (NLP) models and tools. It offers various pretrained models for various NLP tasks, including text classification, question answering, and language translation.\n",
    "\n",
    "One of the key features of Hugging Face Transformers is its support for **multimodal learning**, which combines text and image data for tasks such as image captioning and visual question answering. This capability is particularly relevant to the discussion of **Bootstrapping Language-Image Pretraining (BLIP)**, as it leverages both text and image data to enhance AI models' understanding and generation of image descriptions.\n",
    "\n",
    "In this reading, we'll explore how to use Hugging Face Transformers, specifically the **BLIP model**, for image captioning in Python. We'll demonstrate how to load pretrained models, process images, and generate captions, showcasing the library's capabilities in bridging the gap between natural language and visual content.\n",
    "\n",
    "---\n",
    "\n",
    "#### Introduction to BLIP\n",
    "\n",
    "**BLIP** represents a significant advancement in the intersection of natural language processing (NLP) and computer vision. BLIP, designed to improve AI models, enhances their ability to understand and generate image descriptions. It learns to associate images with relevant text, allowing it to generate captions, answer image-related questions, and support image-based search queries.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why BLIP Matters\n",
    "\n",
    "BLIP is crucial for several reasons:\n",
    "\n",
    "- **Enhanced understanding:** It provides a more nuanced understanding of the content within images, going beyond object recognition to comprehend scenes, actions, and interactions.  \n",
    "- **Multimodal learning:** By integrating text and image data, BLIP facilitates multimodal learning, which is closer to how humans perceive the world.  \n",
    "- **Accessibility:** Generating accurate image descriptions can make content more accessible to people with visual impairments.  \n",
    "- **Content creation:** It supports creative and marketing endeavors by generating descriptive texts for visual content, saving time and enhancing creativity.\n",
    "\n",
    "---\n",
    "\n",
    "#### Real-Time Use Case: Automated Photo Captioning\n",
    "\n",
    "A practical application of BLIP is in developing an automated photo captioning system. Such a system can be used in diverse domains. It enhances social media platforms by suggesting captions for uploaded photos automatically. It also aids digital asset management systems by offering searchable descriptions for stored images.\n",
    "\n",
    "---\n",
    "\n",
    "#### Getting Started with BLIP on Hugging Face\n",
    "\n",
    "Hugging Face offers a platform to experiment with BLIP and other AI models. Below is an example of how to use BLIP for image captioning in Python.\n",
    "\n",
    "Ensure you have Python and the `transformers` library installed.  \n",
    "If not, you can install it using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install transformers Pillow torch torchvision torchaudio\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the processor and model from Hugging Face\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"path_to_your_image.jpg\")\n",
    "\n",
    "# Prepare the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate captions\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "```\n",
    "\n",
    "#### Visual Question Answering\n",
    "\n",
    "BLIP can also answer questions about the content of an image. Here’s an example:\n",
    "```bash\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load BLIP processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# Image URL \n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Specify the question you want to ask about the image\n",
    "question = \"What is in the image?\"\n",
    "\n",
    "# Use the processor to prepare inputs for VQA (image + question)\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the answer from the model\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the answer to the question\n",
    "answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(f\"Answer: {answer}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d83fd6",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name,intensity):\n",
    "    return  \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\",\"slider\"],\n",
    "    outputs=[\"text\"]\n",
    ")\n",
    "\n",
    "demo.launch(server_name=\"127.0.0.1\",server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279afef5",
   "metadata": {},
   "source": [
    "#### Understanding the `Interface` Class\n",
    "\n",
    "Note that to make your first demo, you created an instance of the `gr.Interface` class.  \n",
    "The `Interface` class is designed to create demos for machine learning models that accept one or more inputs and return one or more outputs.\n",
    "\n",
    "---\n",
    "\n",
    "#### Core Arguments of the `Interface` Class\n",
    "\n",
    "The `Interface` class has three core arguments:\n",
    "\n",
    "- **`fn`**: The function to wrap a user interface (UI) around.  \n",
    "- **`inputs`**: The Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.  \n",
    "- **`outputs`**: The Gradio component(s) to use for the output. The number of components should match the number of return values from your function.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Function Argument (`fn`)\n",
    "\n",
    "The `fn` argument is flexible — you can pass any Python function you want to wrap with a UI.  \n",
    "In the example above, you saw a relatively simple function, but the function could be anything from a **music generator** to a **tax calculator** to the **prediction function of a pretrained machine learning model**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Input and Output Components\n",
    "\n",
    "The `inputs` and `outputs` arguments take one or more **Gradio components**.  \n",
    "Gradio includes more than **30 built-in components** (such as `gr.Textbox()`, `gr.Image()`, and `gr.HTML()`) that are designed for machine learning applications.\n",
    "\n",
    "If your function accepts more than one argument, as is the case above, pass a **list of input components** to `inputs`, with each input component corresponding to one of the function's arguments in order.  \n",
    "The same applies if your function returns more than one value: simply pass a list of components to `outputs`.\n",
    "\n",
    "This flexibility makes the `Interface` class a **very powerful way to create demos**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: Image Captioning Model\n",
    "\n",
    "Let's create a simple interface for an image captioning model.  \n",
    "The **BLIP (Bootstrapped Language Image Pretraining)** model can generate captions for images.  Here's how you can create a Gradio interface for the BLIP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed71e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_fast=True)\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image):\n",
    "    # Now directly using the PIL Image object\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def caption_image(image):\n",
    "    \"\"\"\n",
    "    Takes a PIL Image input and returns a caption.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        caption = generate_caption(image)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=caption_image,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Image Captioning with BLIP\",\n",
    "    description=\"Upload an image to generate a caption.\"\n",
    ")\n",
    "\n",
    "iface.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d821b5",
   "metadata": {},
   "source": [
    "#### Image Classification in PyTorch\n",
    "Now let`s explore a different kind of computer vision task — Image Classification. Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n",
    "\n",
    "Such models are perfect to use with Gradio's image input component. In this tutorial, we will build a web demo to classify images using Gradio. We can build the whole web application in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19eb1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Download human-readable labels for ImageNet.\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "labels = response.text.split(\"\\n\")\n",
    "\n",
    "def predict(inp):\n",
    " inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
    " with torch.no_grad():\n",
    "  prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
    "  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n",
    " return confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39e22c",
   "metadata": {},
   "source": [
    "Let's break this down. The function takes one parameter:\n",
    "\n",
    "```inp: the input image as a PIL image```\n",
    "\n",
    "The function converts the input image into a PIL Image and subsequently into a PyTorch tensor. After processing the tensor through the model, it returns the predictions in the form of a dictionary named confidences. The dictionary's keys are the class labels, and its values are the corresponding confidence probabilities.\n",
    "\n",
    "In this section, we define a predict function that processes an input image to return prediction probabilities. The function first converts the image into a PyTorch tensor and then forwards it through the pretrained model. We use the softmax function in the final step to calculate the probabilities of each class. The softmax function is crucial because it converts the raw output logits from the model, which can be any real number, into probabilities that sum up to 1. This makes it easier to interpret the model's outputs as confidence levels for each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08ae27",
   "metadata": {},
   "source": [
    "Now that we have our predictive function set up, we can create a Gradio Interface around it.\n",
    "\n",
    "In this case, the input component is a drag-and-drop image component. To create this input, we use ```Image(type=“pil”)``` which creates the component and handles the preprocessing to convert that to a PIL image.\n",
    "\n",
    "The output component will be a Label, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 classes by constructing it as ```Label(num_top_classes=3)```.\n",
    "\n",
    "Finally, we'll add one more parameter, the examples parameter, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(fn=predict,\n",
    "       inputs=gr.Image(type=\"pil\"),\n",
    "       outputs=gr.Label(num_top_classes=3),\n",
    "       examples=[\"C:\\\\Users\\\\juann\\\\Building-Generative-AI-Powered-Applications-with-Python\\\\Project1\\\\images\\\\UNAL.png\"]).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c6301",
   "metadata": {},
   "source": [
    "## Project 2: My own ChatGPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019292f",
   "metadata": {},
   "source": [
    "With the advent of artificial intelligence or AI, it's now possible to hold an intelligent conversation with a machine. You can extract information on any subject from a computer and save time and effort researching a query. Such as, how do I make an HTTP request in JavaScript? How is this possible? This type of assistance is possible through a computer program or chatbot. \n",
    "\n",
    "A chatbot is a computer program that simulates written or spoken human conversation. With the integration of generative AI technology such as natural language processing or NLP, chatbots can understand questions and respond based on their collected data. The chatbot program takes text as input and delivers a corresponding text as output. A special program called a transformer acts as the brain of the chatbot. The transformer comprises a large language model or LLM, that helps the chat bot understand the input question and generate the human like response as the output. The LLM program goes through the data it has collected and creates a response based on machine learning. The transformer manages the technical processing of input and output data, and the LLM focuses on language comprehension and generation. \n",
    "\n",
    "To build the chatbot, you must select an LLM based on the chatbot's purpose. For example, consider using GPT-2 or GPT-3 models for general purpose text generation, BRT for sentiment analysis, and T-5 for language translation. Other important parameters for choosing an LLM include licensing, model size, training data, and performance and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffb90a",
   "metadata": {},
   "source": [
    "### Flask\n",
    "\n",
    "Flask is a micro-web framework written in Python. It is called a micro-framework because it does not require tools or libraries. However, it supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies, and several common framework-related tools. This flexibility makes Flask adaptable to development needs and serves as the foundation for web applications ranging from small projects to complex, data-driven sites.\n",
    "\n",
    "#### Key Features of Flask\n",
    "\n",
    "* Simplicity: Flask's simple and easy-to-understand syntax makes it accessible for beginners in web development and powerful enough for experienced developers to build robust applications.\n",
    "* Flexibility: The framework can be scaled up with extensions to add features like database integration, authentication, and file upload capabilities.\n",
    "* Development server and debugger: Flask has a built-in development server and a debugger. The development server is lightweight and easy to use, making it ideal for the development and testing phases.\n",
    "* Integrated support for unit testing: Flask supports unit testing out of the box, allowing developers to verify the correctness of their code through tests, ensuring app reliability.\n",
    "* RESTful request dispatching: Flask provides developers with the tools to easily create RESTful APIs, which are crucial for modern web applications and mobile backend services.\n",
    "* Jinja2 templating: Flask uses Jinja2 templating, making creating dynamic web pages with HTML easy. Jinja2 is powerful and flexible, providing security features like template inheritance and automatic HTML escaping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f116b",
   "metadata": {},
   "source": [
    "### Intro: How does a chatbot work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3659f",
   "metadata": {},
   "source": [
    "A chatbot is a computer program that takes a text input, and returns a corresponding text output.\n",
    "\n",
    "Chatbots use a special kind of computer program called a transformer, which is like its brain. Inside this brain, there is something called a language model (LLM), which helps the chatbot understand and generate human-like responses. It deciphers many examples of human conversations it has seen prior to responding in a sensible manner.\n",
    "\n",
    "Transformers and LLMs work together within a chatbot to enable conversation. Here's a simplified explanation of how they interact:\n",
    "\n",
    "* Input processing: When you send a message to the chatbot, the transformer helps process your input. It breaks down your message into smaller parts and represents them in a way that the chatbot can understand. Each part is called a token.\n",
    "\n",
    "* Understanding context: The transformer passes these tokens to the LLM, which is a language model trained on lots of text data. The LLM has learned patterns and meanings from this data, so it tries to understand the context of your message based on what it has learned.\n",
    "\n",
    "* Generating response: Once the LLM understands your message, it generates a response based on its understanding. The transformer then takes this response and converts it into a format that can be easily sent back to you.\n",
    "\n",
    "* Iterative conversation: As the conversation continues, this process repeats. The transformer and LLM work together to process each new input message, understand the context, and generate a relevant response.\n",
    "\n",
    "The key is that the LLM learns from a large amount of text data to understand language patterns and generate meaningful responses. The transformer helps with the technical aspects of processing and representing the input/output data, allowing the LLM to focus on understanding and generating language.\n",
    "\n",
    "Once the chatbot understands your message, it uses the language model to generate a response that it thinks will be helpful or interesting to you. The response is sent back to you, and the process continues as you have a back-and-forth conversation with the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24b644",
   "metadata": {},
   "source": [
    "### Choosing a model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f9b35",
   "metadata": {},
   "source": [
    "Choosing the right model for your purposes is an important part of building chatbots! You can read on the different types of models available on the Hugging Face website: https://huggingface.co/models.\n",
    "\n",
    "LLMs differ from each other in how they are trained. Let's look at some examples to see how different models fit better in various contexts.\n",
    "\n",
    "* Text generation:\n",
    "If you need a general-purpose text generation model, consider using the GPT-2 or GPT-3 models. They are known for their impressive language generation capabilities.\n",
    "Example: You want to build a chatbot that generates creative and coherent responses to user input.\n",
    "\n",
    "* Sentiment analysis:\n",
    "For sentiment analysis tasks, models like BERT or RoBERTa are popular choices. They are trained to understand the sentiment and emotional tone of text.\n",
    "Example: You want to analyze customer feedback and determine whether it is positive or negative.\n",
    "\n",
    "* Named entity recognition:\n",
    "LLMs such as BERT, GPT-2, or RoBERTa can be used for Named Entity Recognition (NER) tasks. They perform well in understanding and extracting entities like person names, locations, organizations, etc.\n",
    "Example: You want to build a system that extracts names of people and places from a given text.\n",
    "\n",
    "* Question answering:\n",
    "Models like BERT, GPT-2, or XLNet can be effective for question-answering tasks. They can comprehend questions and provide accurate answers based on the given context.\n",
    "Example: You want to build a chatbot that can answer factual questions from a given set of documents.\n",
    "\n",
    "* Language translation:\n",
    "For language translation tasks, you can consider models like MarianMT or T5. They are designed specifically for translating text between different languages.\n",
    "Example: You want to build a language translation tool that translates English text to French.\n",
    "\n",
    "However, these examples are very limited and the fit of an LLM may depend on many factors such as data availability, performance requirements, resource constraints, and domain-specific considerations. It's important to explore different LLMs thoroughly and experiment with them to find the best match for your specific application.\n",
    "\n",
    "Other important purposes that should be taken into consideration when choosing an LLM include (but are not limited to):\n",
    "\n",
    "* Licensing: Ensure you are allowed to use your chosen model the way you intend\n",
    "* Model size: Larger models may be more accurate, but might also come at the cost of greater resource requirements\n",
    "* Training data: Ensure that the model's training data aligns with the domain or context you intend to use the LLM for\n",
    "* Performance and accuracy: Consider factors like accuracy, runtime, or any other metrics that are important for your specific use case\n",
    "\n",
    "To explore all the different options, check out the available models on the Hugging Face website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b2079",
   "metadata": {},
   "source": [
    "## Project 3: Voice assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ba7a3",
   "metadata": {},
   "source": [
    "### Docker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5fd11",
   "metadata": {},
   "source": [
    "Available since 2013, the official Docker definition, paraphrased, states that Docker is an open platform for developing, shipping, and running applications as containers. Docker became popular with developers because of its simple architecture, massive scalability, and portability on multiple platforms, environments, and locations. \n",
    "\n",
    "* Docker isolates applications from infrastructure, including the hardware, the operating system, and the container runtime. \n",
    "* Docker is written in the Go programming language Docker uses Linux kernel features to deliver its functionality. \n",
    "* Docker also uses namespaces to provide an isolated workspace called the container. \n",
    "* Docker creates a set of namespaces for every container and each aspect runs in a separate namespace with access limited to that namespace. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa15b8",
   "metadata": {},
   "source": [
    "Docker offers the following benefits: Docker’s consistent and isolated environments result in stable application deployments. Deployments occur in seconds. Because Docker images are small and reusable, they significantly speed up the development process. And, Docker automation capabilities help eliminate errors, simplifying the maintenance cycle. Docker supports Agile and CI/CD DevOps practices. \n",
    "Docker’s easy versioning speeds up testing, rollbacks, and redeployments. Docker helps segment applications for easy refresh, cleanup, and repair. Developers collaborate to resolve issues faster and scale containers when needed. And, Docker images are platform-independent, so they are highly portable. Docker is not a good fit for applications with these qualities: Require high performance or security, are based on Monolith architecture, use rich GUI features, or perform standard desktop or limited functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36852d4b",
   "metadata": {},
   "source": [
    "### IBM Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0afd7",
   "metadata": {},
   "source": [
    "In the world of generative AI, two pivotal technologies that stand out for AI-driven communication are IBM Watson Speech-to-Text (STT) and IBM Watson Text-to-Speech (TTS). These technologies serve as bridges between human language and computer processing, allowing for seamless interactions between users and AI-powered applications.\n",
    "\n",
    "#### IBM Watson speech-to-text (STT)\n",
    "\n",
    "IBM Watson STT is an AI service that converts spoken language into written text using advanced machine learning techniques to for developing applications that require voice input, such as voice-controlled assistants, transcribing meetings, or enhancing customer support with voice commands. In STT, deep neural networks process audio signals to transcribe spoken words into text. These models are trained on diverse datasets, including different languages, accents, and speech in various environments, to improve recognition accuracy and adaptability.\n",
    "\n",
    "##### Key features:\n",
    "STT offers a range of services to enhance AI applications:\n",
    "\n",
    "* Real-time speech recognition: Watson STT can transcribe live audio as it's being spoken, which is crucial for interactive applications.\n",
    "* Language and dialect support: STT supports multiple languages and dialects, making it versatile for global applications.\n",
    "* Customization: Users can train the service with domain-specific terminology and speech patterns, improving accuracy for niche applications.\n",
    "\n",
    "#### IBM Watson text-to-speech (TTS)\n",
    "\n",
    "IBM Watson text-to-speech (TTS) complements the STT service by converting written text into natural-sounding spoken audio. Watson's TTS is among the leading services that produce lifelike and expressive voice outputs. TTS technology has evolved from simple, robotic-sounding outputs to generating speech that closely mimics human tones and inflections. This is achieved through advanced deep learning models that understand text context, emotional cues, and linguistic nuances.\n",
    "\n",
    "##### Key features:\n",
    "\n",
    "- Expressive and natural voices: TTS offers a variety of voices and languages that help to deliver output in accordance with user preferences.\n",
    "- Emotion and expressiveness: TTS allows users to control the tone, emotion, and expressiveness of the voice output to suit the context of the conversation.\n",
    "- Customization: Like STT, Watson TTS allows customization of voices and can be trained to include specific jargon or pronunciations unique to a business or industry.\n",
    "\n",
    "#### Integrating Watson's STT and TTS in AI Projects\n",
    "\n",
    "Integrating STT and TTS services into AI projects can significantly enhance the user experience by enabling natural and intuitive interactions.\n",
    "\n",
    "Here are some steps and considerations for integrating IBM Watson's speech services into applications:\n",
    "\n",
    "* API keys and IBM cloud setup: STT and TTS services are accessible via the IBM Cloud platform accessible through an IBM Cloud account. Users will need to create instances for STT and TTS services and obtain API keys for authentication.\n",
    "* Choosing the right SDK: IBM offers SDKs for various programming languages, including Python, which facilitates the integration of these services into applications.\n",
    "* Understanding the APIs: Familiarize yourself with the API documentation for STT and TTS, as understanding the available methods, parameters, and response formats is crucial for effective integration.\n",
    "* Designing user interactions: Consider how users will interact with the application through voice. Designing a smooth and intuitive voice UI is key to a successful voice-enabled application.\n",
    "* Handling audio data: Ensure your application can correctly capture audio data for STT and process TTS audio output that is compatible with the application's front end.\n",
    "* Privacy and security: When dealing with voice data, especially in applications that may handle sensitive information, it's essential to consider privacy and security measures to protect user data\n",
    "\n",
    "#### The future of voice-enabled technologies\n",
    "\n",
    "As STT and TTS technologies continue to advance, their potential to revolutionize human-computer interaction grows. Future developments may focus on further improving the naturalness and expressiveness of TTS outputs and enhancing the accuracy and adaptability of STT in complex environments like:\n",
    "\n",
    "* Emotional intelligence: Future TTS systems may incorporate more sophisticated emotional intelligence, allowing them to adjust tone and expressiveness based on the conversational context or the user's emotional state, detected through speech patterns and linguistic cues.\n",
    "* Contextual understanding: Enhancing STT with better contextual understanding could lead to more accurate transcription and interpretation of speech, considering the broader context of conversations or the specific domain of application.\n",
    "* Cross-platform integration: As voice-enabled technologies become more pervasive, seamless integration across different platforms and devices will be crucial. This could enable a more unified user experience, allowing continuous, context-aware interactions regardless of the device or application used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d2fab",
   "metadata": {},
   "source": [
    "## Project 4: Generative AI-Powered Meeting Assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72898e0c",
   "metadata": {},
   "source": [
    "Imagine you're in a brainstorming meeting, ideas flying fast and furious, and nuggets of information are being exchanged. You jot down notes, but can you capture everything? Later, recalling key decisions or specific action items becomes frustrating. This is where an innovative, generative AI-based app that can transcribe and summarize a meetings discussions would be helpful. Imagine an app that can transcribe the meeting discussions accurately and then provide a concise summary, highlighting the key points and decisions made. This is the power of combining automatic speech recognition, or ASR, and generative AI-based large language models or LLMs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fee49",
   "metadata": {},
   "source": [
    "### LLama 2\n",
    "\n",
    "Llama 2 is a family of pre-trained and fine-tuned large language models (LLMs) released by Meta AI. Llama 2 AI models are capable of a variety of natural language processing (NLP) tasks, from text generation to programming code.\n",
    "\n",
    "The original Llama model was a landmark in the generative AI landscape, offering capabilities for understanding and generating human-like text. It was designed to support various applications, from simple question-answering systems to more complex natural language processing tasks.\n",
    "\n",
    "Building upon the solid foundation laid by its predecessors, such as Generative Pretrained Transformer (GPT) and Llama, Meta Llama 2 introduces significant enhancements in understanding and generating human-like text. These advancements mean that Meta Llama 2 can handle more data, generate more accurate and coherent responses, and do so more quickly and with fewer computational resources than the original Llama model. These improvements support better comprehension of complex queries, more nuanced and contextually appropriate text generation, and support for a broader array of languages.\n",
    "\n",
    "By leveraging the latest advancements in AI research and technology, Meta Llama 2 aims to surpass the capabilities of earlier models like GPT-3, Bidirectional Encoder Representations from Transformers (BERT), and the original Llama, setting a new standard in efficiency and versatility for natural language processing tasks.\n",
    "\n",
    "#### Applications of Meta Llama 2\n",
    "The versatility of Meta Llama 2 enables its application across a wide array of fields:\n",
    "\n",
    "* Content creation: Llama 2 can assist or automate content creation processes, from writing articles to generating creative fiction.\n",
    "* Data analysis and summarization: It can quickly analyze large volumes of text and provide concise summaries, extracting key insights from data.\n",
    "* Language translation: With its multilingual capabilities, Llama 2 facilitates seamless translation between \n",
    "* languages, enhancing communication across cultures.\n",
    "* Educational tools: The model can power tutoring systems, offering personalized learning experiences and explanations to students.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f4021",
   "metadata": {},
   "source": [
    "### OpenAI Whisper\n",
    "\n",
    "OpenAI Whisper is a revolutionary speech recognition system designed to transcribe audio into text accurately. Built on a deep learning model, Whisper demonstrates remarkable proficiency in handling a wide range of audio types, from clear studio recordings to noisy environments, and supports multiple languages. This flexibility makes Whisper an invaluable tool for developers looking to integrate voice functionalities into their applications.\n",
    "\n",
    "* High accuracy: Whisper's accuracy is one of its standout features, achieved through training on a diverse and extensive data set. This data set includes various audio types and scenarios, enabling Whisper to handle various speech patterns, accents, and dialects precisely. With its deep learning foundation, Whisper can understand context, improving its ability to accurately transcribe even when audio quality is compromised or speech is unclear.\n",
    "* Multilingual support: Whisper can recognize and transcribe speech in numerous languages. Trained on audio samples from a vast array of languages, it supports multilingual transcription without requiring manual language selection. This makes it versatile for global applications, facilitating communication and content creation across different linguistic communities.\n",
    "* Robustness to noise: A critical advantage of Whisper is its robustness in noisy environments. Its training includes a variety of noisy audio samples, which helps the model effectively distinguish speech from background noise. This feature is particularly beneficial for applications in challenging acoustic conditions, ensuring reliable transcription in situations ranging from busy street interviews to lively public gatherings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a687a42",
   "metadata": {},
   "source": [
    "## Project 5: RAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c3d69",
   "metadata": {},
   "source": [
    "### LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c9068",
   "metadata": {},
   "source": [
    "The LangChain open-source Python framework streamlines the development of large language model (LLM) applications. LangChain provides developers with components and interfaces to assist in integrating LLMs into their AI applications. The framework is used to pinpoint relevant information in text, such as a research paper or legal document. It also provides methods for responding to complex prompts by retrieving data and generating a coherent summary. It chains together the retrieval, extraction, processing, and generation operations from the large amounts of text and multiple sources, hence the word chain as part of its name. \n",
    "AI developers prefer this framework because of several key benefits, including its modularity, extensibility, and decomposition capabilities. It also easily integrates with vector databases.\n",
    "\n",
    "* LangChain's design allows application developers to piece together different components like building blocks. This modularity also encourages component reuse, reducing development time and effort. \n",
    "\n",
    "* LangChain's extensible design allows developers to readily add new features, adapt to existing components, integrate with external systems, and make only minimal changes to their codebases. \n",
    "\n",
    "* LangChain mimics the human problem-solving process by breaking down complex queries or tasks into smaller, manageable steps. This decomposition capability enables it to make accurate inferences from context, resulting in relevant, precise responses. \n",
    "\n",
    "* LangChain integrates with vector databases for efficient semantic searches and information retrieval. When used in conjunction with vector databases, it provides applications quick access to relevant information within extensive data sets.\n",
    "\n",
    "AI applications can leverage the LangChain framework for a number of practical uses such as content summarization, data extraction, sophisticated question and answer systems, and automated content generation.\n",
    "\n",
    "With its ability to summarize articles, reports, and documents, users can stay better informed by deciphering the meaning of complex legal documents. It can extract key statistics from reports, simplifying the process of turning text into actionable insights. \n",
    "\n",
    "* Question and answer systems with LangChain can transform customer support and knowledge-based services. These systems can provide contextually relevant answers to a chain of clarifying responses based on the entire conversation. \n",
    "\n",
    "* LangChain can assist with the generation of written materials. The framework opens possibilities for automating routine writing tasks such as drafting emails, brainstorming, or technical documentation. \n",
    "\n",
    "While primarily designed for text based applications, LangChain can work with other data types such as images, audio, and video by leveraging external libraries and models like speech to text. Its integration with vector databases enables the use of embeddings generated from these data types to capture semantic meaning and perform similarity searches, making it a valuable tool for various AI tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1c9a1",
   "metadata": {},
   "source": [
    "### Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba051427",
   "metadata": {},
   "source": [
    "Llama 2 is a state-of-the-art large language model developed as part of the Llama suite of models. It is designed to understand and generate human-like text by training on large volumes of data. Llama 2 uses Natural Language Processing (NLP) techniques for text completion, summarization, answering human questions, and more.\n",
    "\n",
    "#### Benefits\n",
    "Llama 2 is useful for several reasons.\n",
    "\n",
    "* Ability to produce content: Llama 2 can understand the questions' context and produce relevant content.\n",
    "* Easy integration: Llama 2 is a secure application that is easy to integrate with other advanced NLP functionalities.\n",
    "* Code generation: Llama 2 can generate codes and natural language about code from natural and code prompts.\n",
    "\n",
    "#### Limitations\n",
    "Despite its strengths, deploying Llama 2 in a public setting poses several challenges.\n",
    "\n",
    "* Compromised data privacy and security: Llama 2 is a public LLM that may not offer the same level of data privacy and security that is critical for handling sensitive or proprietary information. Highlighting concerns around data breaches or unauthorized access is crucial to protecting confidential data, which may not be possible with Llama 2.\n",
    "* Limited customization: Being a public LLM, Llama 2 users may have limited ability to customize the model and its outputs to suit specific business needs or domain-specific requirements, affecting the relevance and accuracy of the model's responses.\n",
    "* Slow resource sharing and performance: Llama is usually shared among multiple users, leading to variable performance and response times unsuitable for applications requiring consistent and high-speed processing.\n",
    "* High cost: Llama 2 can sometimes be more costly than hosting privately for high-volume or commercial use, especially when scaling up operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92e93c",
   "metadata": {},
   "source": [
    "### RAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099ddf8",
   "metadata": {},
   "source": [
    "Retrieval-augmented generation (RAG) is a sophisticated technique that enhances LLMs by integrating external information retrieval into the text generation process. RAG represents a significant leap forward in making AI-generated content more contextually aware and precise.\n",
    "\n",
    "#### Benefits\n",
    "RAG is widely used because of its extensive capabilities.\n",
    "\n",
    "* Intelligent model response: RAG offers accurate and relevant responses by dynamically incorporating additional information on which the model was not trained.\n",
    "* Auto update: RAG reduces the need for users to continuously train the model on new data and update its parameters based on the given conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f82e68",
   "metadata": {},
   "source": [
    "### LangChain to implement RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c7d9e",
   "metadata": {},
   "source": [
    "LangChain is a framework designed to simplify the integration of language models with other technologies, such as databases and knowledge bases, for applications like RAG.\n",
    "\n",
    "It provides tools and abstractions that make it easier to build complex NLP applications without directly managing the intricacies of model architecture or information retrieval systems.\n",
    "\n",
    "#### The process to implement RAG with Llama 2\n",
    "* Configure the model: This involves setting up Llama 2 within LangChain and specifying parameters such as the model size and the interface for interacting with it.\n",
    "* Integrate data source: Connect the model to external data sources or knowledge bases from which it can retrieve information during the generation process. LangChain supports various data source types, ensuring flexibility in integrating knowledge.\n",
    "* Generate pipeline: Defining the pipeline for generating responses, including the retrieval query, processing the retrieved data, and incorporating this information into the model's generative process.\n",
    "By abstracting the complexity of combining language models with retrieval systems, LangChain helps leverage RAG's full potential with Llama 2, creating applications that generate highly accurate, information-rich responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
