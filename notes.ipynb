{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45499be",
   "metadata": {},
   "source": [
    "# Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001f9d0",
   "metadata": {},
   "source": [
    "## Generative AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030732f4",
   "metadata": {},
   "source": [
    "In the world of generative AI, these four models have made a significant impact. Variational autoencoders, generative adversarial networks, transformer-based models, and diffusion models. Each model employs a different type of deep learning architecture and applies probabilistic techniques. Let's gain insight into how they work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1efe8a",
   "metadata": {},
   "source": [
    "### Variational autoencoders\n",
    "\n",
    "Variational autoencoders or VAEs are the most popular of all generative AI models for two reasons. \n",
    "-   They work with a diverse range of training data, such as images, text, and audio. \n",
    "- They rapidly reduce the dimensionality of your image, text, or audio to create a newer improved version.\n",
    "\n",
    "\n",
    "First, the encoder, which is a self-sufficient neural network, studies the probability distribution of the input data. In simple terms, this means that it isolates the most useful data variables. This allows the encoder to create a compressed representation of the data sample and store it in the latent space. You can think of this latent space as a mathematical space within the model's architecture, where large dimensional data is represented in a compressed format. Next, the decoder or reverse encoder, which is also a self-sufficient neural network, decompresses the compressed representation in the latent space to generate the desired output. Basically, the algorithms are trained using a maximum likelihood principle, which means they try to minimize the difference between the original input data and the reconstructed output. \n",
    "\n",
    "Although VAEs are trained in a static environment, their latent space is characterized as continuous. Therefore, they can generate new samples by randomly sampling from the probability distribution of data. Because they can produce realistic and varied images with little training data, VAEs are used in image synthesis, data compression, and anomaly detection tasks. For example, the entertainment industry uses VAEs to create game maps and animate avatars. The finance industry uses VAEs to forecast the volatility surfaces of stocks. The healthcare sector uses VAEs to detect diseases using electrocardiogram signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474b542",
   "metadata": {},
   "source": [
    "### GANs\n",
    "\n",
    "Generative adversarial network organ is another type of generative AI model that uses imagery and textual input data. In this model, two convolutional neural networks or CNNs, compete with each other in an adversarial game. One CNN plays the role of a generator and is trained on a vast dataset to produce data samples. The other CNN plays the role of a discriminator and tries to distinguish between real and fake samples. Based on the discriminator's responses, the generator seeks to produce more realistic data samples. GANs can generate new realistic-looking images, perform a style transfer or image to image translation and even create deep fakes. The finance industry uses GANs to train models for loan pricing or generating time series. \n",
    "Tools such as SpaceGAN work with geospatial data and videos StyleGAN2 is known for creating video game characters. Unlike variational autoencoders, GANs can be challenging to train as they require a large amount of data and heavy computational power. They can potentially create false material which is an ethical concern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d025fbf",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Transformer-based models were introduced a few years ago when recurrent neural networks or RNN started facing a problem called vanishing gradients. Due to this problem, RNNs were struggling to process long sequences of text. To get around this challenge, transformers were built with attention mechanisms that could focus on the most valuable parts of the text while filtering out the unnecessary elements. This allowed transformers to model long-term dependencies in text. \n",
    "For instance, when you enter a simple prompt, the two-stack transformer architecture uses an encoder-decoder mechanism to generate coherent and contextually relevant text. As transformer models can query extensive databases, they are able to create large language models and perform natural language processing tasks such as picture creation, music synthesis, and even video synthesis. This marks a significant breakthrough in our approach to content creation and offers many opportunities for innovation as has been seen with GPT 3.5 and its subsequent versions, BERT and T5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b0774",
   "metadata": {},
   "source": [
    "### Difusion models\n",
    "\n",
    "Diffusion models are a more recent addition to the world of generative AI models. They address the systematic decay of data that occurs due to noise in the latent space. By applying the principles of diffusion, these models try to prevent information loss. Just as in the diffusion process, where molecules move from high-density to low-density areas, diffusion models move noise to and from a data sample using a two-step process. \n",
    "Step 1 is forward diffusion, in which algorithms gradually add random noise to training data. Step 2 is reverse diffusion, in which algorithms turn the noise around to recover the data and generate the desired output. Open AI's Dall-E2, Stability AI's Stable Diffusion XL, and Google's Imagen are mature diffusion models that generate high-quality graphical content. Similar to variational autoencoders, diffusion models also try to optimize data by first projecting it onto the latent space and then recovering it back to the initial state. However, a diffusion model is trained using a dynamic flow and therefore takes longer to train. Then why are these models considered the best option for creating generative AI models? Because they train hundreds, maybe even an unlimited number of layers and have shown remarkable results in image synthesis and video generation. \n",
    "Experiments with generative AI models continue unabated as unsupervised algorithms throw up one surprise after another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb52db",
   "metadata": {},
   "source": [
    "## Fundation models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523b452",
   "metadata": {},
   "source": [
    "Stanford University Center for Research on Foundation Models defines a foundation model as a new successful paradigm for building AI systems. Train one model on a huge amount of data and adapt it to many applications. We call such a model a foundation model. Let's explore this definition more closely. The first part of this definition says train one model on a huge amount of data. How does this work? \n",
    "A foundation model is a large general purpose self supervised model that is pre-trained on vast amounts of unlabeled data, establishing billions of parameters. Pre-training is a technique during which unsupervised algorithms are repeatedly given the liberty to make connections between diverse pieces of information. This allows foundation models to develop multimodal, multi-domain capabilities. Such that they can accept input prompts and multiple modalities such as text, image, audio, or video formats and perform complex and creative tasks, such as answering questions, summarizing documents, writing essays, solving equations, extracting information from images, even developing code. This broad skill set makes these models relevant to multiple domains. This is in contrast the smaller generative AI models, which are trained on restricted domain data and requested to perform limited tasks. For instance, OpenAI's Dall-E family of models are considered foundation models because they can perform many image related tasks. \n",
    "In contrast, AlexNet is not classified as a foundational model as it only performs image classification tasks. Therefore, we can clarify that while all foundation models have generative AI capabilities, not all generative AI models are foundation models. When foundation models are trained on vast natural language processing databases, they are called Large Language Models or LLMs. LLMs develop independent reasoning allowing them to respond to queries uniquely, for example, OpenAI's, GPT and class of models including GPT-3, which is pre-trained on 175 plus billion parameters and GPT-4, which is pre-trained on an estimated 180 plus trillion parameters. Other examples of large language models include Google's Pathway Language Model pre-trained on 540 billion parameters, Meta's Large Language Model Meta AI pre trained on 65 billion parameters. Google's Bert pre-trained on 340 million plus parameters. Meta's Galactica, an LLM for scientists pre-trained on 48 million papers, lectures, textbooks, and websites. \n",
    "Technology innovation institutes Falcon 7B pre-trained on 1.5 trillion tokens and Microsoft's Orca pre-trained at 13 billion parameters and small enough to run off a laptop. It's likely that these parameters may change as generative AI tools evolve in their scope and size. Another aspect of models evolving is their ability to adapt. The definition also suggests that we can adapt a foundation model to many applications. This is possible because of the broad based training of foundation models, which allows them to learn new things and adapt to new situations. Small businesses can leverage this capability to create customized, more efficient, generative AI models at an affordable cost. This is why foundation models are also called base models. \n",
    "They help make AI systems more accessible to businesses and individuals who do not have the resources to train their models from scratch. In this way, foundation models enable enterprises to shrink time to value from months to weeks. Take for example the evolution of chatbots. OpenAI's GPT-3 and GPT-4 are foundation models that power the ChatGPT chatbot. Google's PaLM powers the Google barred chatbot. These are today's unreasonably clever chatbots. However, if we think back to how early chatbots functioned, we realize that they were trained on smaller datasets which confine their generative capabilities. \n",
    "While they could predict responses based on keywords, they could only provide a predetermined response. In contrast, chatbots today are pre-trained multiple times on extensive datasets. They are therefore able to increase their word prediction accuracy and respond in a more helpful and creative manner. Try this will you? If you type a single sentence prompt and chatGPT, you'll likely get more than a basic response depending on what your prompt requested. The chatbot may write a comparative essay, create an infographic, design a checklist or script a short story. OpenAI's GPT-3 is also the foundation model for Dall-E. An image generation tool that responds to text prompts. \n",
    "For a single text prompt, Dall-E generates four high resolution images in multiple styles, including photo realistic images and paintings. Another clarification to note here, while all large language models are foundation models, not all foundation models are large language models. Some foundation models use diffusion architecture capabilities to improve the scale and scope of their image generation capabilities. For instance, Dall-E uses transformer architecture. But the latest version of Dall-E uses sound diffusion to generate images from text. Stability, AI stable diffusion uses diffusion architecture to generate high resolution images in realistic cartooning and abstract styles based on the user's description. Google's imaging uses a cascaded diffusion model built on an LLM to generate images from text prompts. \n",
    "As foundation models evolve in their strengths and applications, we have seen some limitations. Firstly, the desired output may be biased if the data on which the foundation model is trained is biased. Secondly, LLMs can hallucinate responses. That means they generate false information because they misinterpret the context of data parameters within a dataset. Therefore, you must verify the accuracy of the output produced by a generative AI chatbot. With a little caution, you can enjoy the many benefits foundation models offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618ea61",
   "metadata": {},
   "source": [
    "## Project 1:Image Captioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e58ca",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "\n",
    "Hugging Face is an open source artificial intelligence platform where scientists, developers and businesses collaborate to build personalized machine learning tools. The platform was built with the purpose of creating a hub for the open source AI community to share models, data sets and applications. This way AI becomes accessible to all types of users, even those who do not have the budget or bandwidth to build machine learning applications independently. Hugging Face is therefore credited with democratizing AI as everyone comes together to benefit from smaller, curated models. Challenging the general one model to rule assumption. \n",
    "At first the hugging phase community focused on creating transformer-based models to leverage the capabilities of natural language processing, or NLP. However, today the platform offers various machine learning tools for generating text, images, audio, and video. Currently, the Hugging Face platform hosts over 250,000 open models, 50,000 data sets, and one million open demos. This list keeps growing. Scientists and developers use Hugging Face to build, train and deploy their AI models. They have access to the platform's open source transformer library, which has over 25,000 pretrained models for PyTorch, Tensorflow, and Google Jax. PyTorch is a deep learning library. \n",
    "Tensorflow is a machine learning platform and Google Jax is a machine learning framework. The models in the library perform varied tasks, such as text generation, question answering, summarization, automatic speech recognition, and image segmentation, just to name a few. Users can filter these models by name to find an existing model or share their own model with the library. Developers can also host demos of generative AI applications on the Spaces tab, allowing users to interact and validate them. How do businesses benefit from the platform? Hugging Face offers businesses the enterprise hub from where they can access pre trained models and data sets. This allows businesses to leverage existing infrastructure rather than build models from scratch. \n",
    "Not only does this reduce their carbon footprint, time, and cost to scale, it also allows businesses to train the models on proprietary data and relevant use cases. Additionally, Hugging Face helps businesses A, add or remove features to improve the efficiency of their models. B, evaluate their generative AI models to filter biased data. C, create multimodal applications with text, image, audio, and video generation capabilities. More than 50,000 large and small companies actively use Hugging Face. For example, Writer, a generative AI solution provider, hosts its Palmera Large Language Models or LLMs on Hugging Face. Intel has officially joined Hugging Face Hardware Partner program and is collaborating with Hugging Face to build state- of-the-art machine learning hardware and end-to-end machine learning workflows. \n",
    "Even universities and non-profits are part of Hugging Face. Among other services. Hugging Face offers an expert acceleration program to guide non-developers on machine learning models. HuggingChat is the first open source alternative to ChatGPT. To protect its users, Hugging Face complies with service organization control type 2 regulation. This means ensuring user data security, availability, processing integrity, confidentiality, and privacy. Taking its collaborative efforts one step further, Hugging Face has entered into a unique partnership with watsonx.ai, IBM's next generation enterprise studio for AI builders. \n",
    "Watsonx.ai offers select Hugging Face models in its studio to help its community of builders train, test and deploy all types of machine learning and generative AI applications. This way, the studio leverages the diversity of data community strength and open source libraries that Hugging Face provides. On its end, Hugging Face creates open source versions of IBM's LLMs and makes them available on a hub. Both entities believe in open source technology, both bet on the community to create value in the AI space. As proprietary AI models can quickly become obsolete, Hugging Face may develop an edge over the big five in AI, namely, Google, Open AI, Meta, IBM, and Microsoft. This is because it supports and is supported by the open source AI community that keeps innovating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ace6a6",
   "metadata": {},
   "source": [
    "### BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf6331",
   "metadata": {},
   "source": [
    "#### Introduction to Hugging Face Transformers\n",
    "\n",
    "Hugging Face Transformers is a popular open-source library that provides state-of-the-art natural language processing (NLP) models and tools. It offers various pretrained models for various NLP tasks, including text classification, question answering, and language translation.\n",
    "\n",
    "One of the key features of Hugging Face Transformers is its support for **multimodal learning**, which combines text and image data for tasks such as image captioning and visual question answering. This capability is particularly relevant to the discussion of **Bootstrapping Language-Image Pretraining (BLIP)**, as it leverages both text and image data to enhance AI models' understanding and generation of image descriptions.\n",
    "\n",
    "In this reading, we'll explore how to use Hugging Face Transformers, specifically the **BLIP model**, for image captioning in Python. We'll demonstrate how to load pretrained models, process images, and generate captions, showcasing the library's capabilities in bridging the gap between natural language and visual content.\n",
    "\n",
    "---\n",
    "\n",
    "#### Introduction to BLIP\n",
    "\n",
    "**BLIP** represents a significant advancement in the intersection of natural language processing (NLP) and computer vision. BLIP, designed to improve AI models, enhances their ability to understand and generate image descriptions. It learns to associate images with relevant text, allowing it to generate captions, answer image-related questions, and support image-based search queries.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why BLIP Matters\n",
    "\n",
    "BLIP is crucial for several reasons:\n",
    "\n",
    "- **Enhanced understanding:** It provides a more nuanced understanding of the content within images, going beyond object recognition to comprehend scenes, actions, and interactions.  \n",
    "- **Multimodal learning:** By integrating text and image data, BLIP facilitates multimodal learning, which is closer to how humans perceive the world.  \n",
    "- **Accessibility:** Generating accurate image descriptions can make content more accessible to people with visual impairments.  \n",
    "- **Content creation:** It supports creative and marketing endeavors by generating descriptive texts for visual content, saving time and enhancing creativity.\n",
    "\n",
    "---\n",
    "\n",
    "#### Real-Time Use Case: Automated Photo Captioning\n",
    "\n",
    "A practical application of BLIP is in developing an automated photo captioning system. Such a system can be used in diverse domains. It enhances social media platforms by suggesting captions for uploaded photos automatically. It also aids digital asset management systems by offering searchable descriptions for stored images.\n",
    "\n",
    "---\n",
    "\n",
    "#### Getting Started with BLIP on Hugging Face\n",
    "\n",
    "Hugging Face offers a platform to experiment with BLIP and other AI models. Below is an example of how to use BLIP for image captioning in Python.\n",
    "\n",
    "Ensure you have Python and the `transformers` library installed.  \n",
    "If not, you can install it using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install transformers Pillow torch torchvision torchaudio\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the processor and model from Hugging Face\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"path_to_your_image.jpg\")\n",
    "\n",
    "# Prepare the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate captions\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "```\n",
    "\n",
    "#### Visual Question Answering\n",
    "\n",
    "BLIP can also answer questions about the content of an image. Here’s an example:\n",
    "```bash\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load BLIP processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# Image URL \n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Specify the question you want to ask about the image\n",
    "question = \"What is in the image?\"\n",
    "\n",
    "# Use the processor to prepare inputs for VQA (image + question)\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the answer from the model\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the answer to the question\n",
    "answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(f\"Answer: {answer}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d83fd6",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name,intensity):\n",
    "    return  \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\",\"slider\"],\n",
    "    outputs=[\"text\"]\n",
    ")\n",
    "\n",
    "demo.launch(server_name=\"127.0.0.1\",server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279afef5",
   "metadata": {},
   "source": [
    "#### Understanding the `Interface` Class\n",
    "\n",
    "Note that to make your first demo, you created an instance of the `gr.Interface` class.  \n",
    "The `Interface` class is designed to create demos for machine learning models that accept one or more inputs and return one or more outputs.\n",
    "\n",
    "---\n",
    "\n",
    "#### Core Arguments of the `Interface` Class\n",
    "\n",
    "The `Interface` class has three core arguments:\n",
    "\n",
    "- **`fn`**: The function to wrap a user interface (UI) around.  \n",
    "- **`inputs`**: The Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.  \n",
    "- **`outputs`**: The Gradio component(s) to use for the output. The number of components should match the number of return values from your function.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Function Argument (`fn`)\n",
    "\n",
    "The `fn` argument is flexible — you can pass any Python function you want to wrap with a UI.  \n",
    "In the example above, you saw a relatively simple function, but the function could be anything from a **music generator** to a **tax calculator** to the **prediction function of a pretrained machine learning model**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Input and Output Components\n",
    "\n",
    "The `inputs` and `outputs` arguments take one or more **Gradio components**.  \n",
    "Gradio includes more than **30 built-in components** (such as `gr.Textbox()`, `gr.Image()`, and `gr.HTML()`) that are designed for machine learning applications.\n",
    "\n",
    "If your function accepts more than one argument, as is the case above, pass a **list of input components** to `inputs`, with each input component corresponding to one of the function's arguments in order.  \n",
    "The same applies if your function returns more than one value: simply pass a list of components to `outputs`.\n",
    "\n",
    "This flexibility makes the `Interface` class a **very powerful way to create demos**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: Image Captioning Model\n",
    "\n",
    "Let's create a simple interface for an image captioning model.  \n",
    "The **BLIP (Bootstrapped Language Image Pretraining)** model can generate captions for images.  Here's how you can create a Gradio interface for the BLIP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed71e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_fast=True)\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image):\n",
    "    # Now directly using the PIL Image object\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def caption_image(image):\n",
    "    \"\"\"\n",
    "    Takes a PIL Image input and returns a caption.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        caption = generate_caption(image)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=caption_image,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Image Captioning with BLIP\",\n",
    "    description=\"Upload an image to generate a caption.\"\n",
    ")\n",
    "\n",
    "iface.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d821b5",
   "metadata": {},
   "source": [
    "#### Image Classification in PyTorch\n",
    "Now let`s explore a different kind of computer vision task — Image Classification. Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n",
    "\n",
    "Such models are perfect to use with Gradio's image input component. In this tutorial, we will build a web demo to classify images using Gradio. We can build the whole web application in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19eb1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Download human-readable labels for ImageNet.\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "labels = response.text.split(\"\\n\")\n",
    "\n",
    "def predict(inp):\n",
    " inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
    " with torch.no_grad():\n",
    "  prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
    "  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n",
    " return confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39e22c",
   "metadata": {},
   "source": [
    "Let's break this down. The function takes one parameter:\n",
    "\n",
    "```inp: the input image as a PIL image```\n",
    "\n",
    "The function converts the input image into a PIL Image and subsequently into a PyTorch tensor. After processing the tensor through the model, it returns the predictions in the form of a dictionary named confidences. The dictionary's keys are the class labels, and its values are the corresponding confidence probabilities.\n",
    "\n",
    "In this section, we define a predict function that processes an input image to return prediction probabilities. The function first converts the image into a PyTorch tensor and then forwards it through the pretrained model. We use the softmax function in the final step to calculate the probabilities of each class. The softmax function is crucial because it converts the raw output logits from the model, which can be any real number, into probabilities that sum up to 1. This makes it easier to interpret the model's outputs as confidence levels for each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08ae27",
   "metadata": {},
   "source": [
    "Now that we have our predictive function set up, we can create a Gradio Interface around it.\n",
    "\n",
    "In this case, the input component is a drag-and-drop image component. To create this input, we use ```Image(type=“pil”)``` which creates the component and handles the preprocessing to convert that to a PIL image.\n",
    "\n",
    "The output component will be a Label, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 classes by constructing it as ```Label(num_top_classes=3)```.\n",
    "\n",
    "Finally, we'll add one more parameter, the examples parameter, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(fn=predict,\n",
    "       inputs=gr.Image(type=\"pil\"),\n",
    "       outputs=gr.Label(num_top_classes=3),\n",
    "       examples=[\"C:\\\\Users\\\\juann\\\\Building-Generative-AI-Powered-Applications-with-Python\\\\Project1\\\\images\\\\UNAL.png\"]).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c6301",
   "metadata": {},
   "source": [
    "## Project 2: My own ChatGPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019292f",
   "metadata": {},
   "source": [
    "With the advent of artificial intelligence or AI, it's now possible to hold an intelligent conversation with a machine. You can extract information on any subject from a computer and save time and effort researching a query. Such as, how do I make an HTTP request in JavaScript? How is this possible? This type of assistance is possible through a computer program or chatbot. \n",
    "\n",
    "A chatbot is a computer program that simulates written or spoken human conversation. With the integration of generative AI technology such as natural language processing or NLP, chatbots can understand questions and respond based on their collected data. The chatbot program takes text as input and delivers a corresponding text as output. A special program called a transformer acts as the brain of the chatbot. The transformer comprises a large language model or LLM, that helps the chat bot understand the input question and generate the human like response as the output. The LLM program goes through the data it has collected and creates a response based on machine learning. The transformer manages the technical processing of input and output data, and the LLM focuses on language comprehension and generation. \n",
    "\n",
    "To build the chatbot, you must select an LLM based on the chatbot's purpose. For example, consider using GPT-2 or GPT-3 models for general purpose text generation, BRT for sentiment analysis, and T-5 for language translation. Other important parameters for choosing an LLM include licensing, model size, training data, and performance and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffb90a",
   "metadata": {},
   "source": [
    "### Flask\n",
    "\n",
    "Flask is a micro-web framework written in Python. It is called a micro-framework because it does not require tools or libraries. However, it supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies, and several common framework-related tools. This flexibility makes Flask adaptable to development needs and serves as the foundation for web applications ranging from small projects to complex, data-driven sites.\n",
    "\n",
    "#### Key Features of Flask\n",
    "\n",
    "* Simplicity: Flask's simple and easy-to-understand syntax makes it accessible for beginners in web development and powerful enough for experienced developers to build robust applications.\n",
    "* Flexibility: The framework can be scaled up with extensions to add features like database integration, authentication, and file upload capabilities.\n",
    "* Development server and debugger: Flask has a built-in development server and a debugger. The development server is lightweight and easy to use, making it ideal for the development and testing phases.\n",
    "* Integrated support for unit testing: Flask supports unit testing out of the box, allowing developers to verify the correctness of their code through tests, ensuring app reliability.\n",
    "* RESTful request dispatching: Flask provides developers with the tools to easily create RESTful APIs, which are crucial for modern web applications and mobile backend services.\n",
    "* Jinja2 templating: Flask uses Jinja2 templating, making creating dynamic web pages with HTML easy. Jinja2 is powerful and flexible, providing security features like template inheritance and automatic HTML escaping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f116b",
   "metadata": {},
   "source": [
    "### Intro: How does a chatbot work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3659f",
   "metadata": {},
   "source": [
    "A chatbot is a computer program that takes a text input, and returns a corresponding text output.\n",
    "\n",
    "Chatbots use a special kind of computer program called a transformer, which is like its brain. Inside this brain, there is something called a language model (LLM), which helps the chatbot understand and generate human-like responses. It deciphers many examples of human conversations it has seen prior to responding in a sensible manner.\n",
    "\n",
    "Transformers and LLMs work together within a chatbot to enable conversation. Here's a simplified explanation of how they interact:\n",
    "\n",
    "* Input processing: When you send a message to the chatbot, the transformer helps process your input. It breaks down your message into smaller parts and represents them in a way that the chatbot can understand. Each part is called a token.\n",
    "\n",
    "* Understanding context: The transformer passes these tokens to the LLM, which is a language model trained on lots of text data. The LLM has learned patterns and meanings from this data, so it tries to understand the context of your message based on what it has learned.\n",
    "\n",
    "* Generating response: Once the LLM understands your message, it generates a response based on its understanding. The transformer then takes this response and converts it into a format that can be easily sent back to you.\n",
    "\n",
    "* Iterative conversation: As the conversation continues, this process repeats. The transformer and LLM work together to process each new input message, understand the context, and generate a relevant response.\n",
    "\n",
    "The key is that the LLM learns from a large amount of text data to understand language patterns and generate meaningful responses. The transformer helps with the technical aspects of processing and representing the input/output data, allowing the LLM to focus on understanding and generating language.\n",
    "\n",
    "Once the chatbot understands your message, it uses the language model to generate a response that it thinks will be helpful or interesting to you. The response is sent back to you, and the process continues as you have a back-and-forth conversation with the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24b644",
   "metadata": {},
   "source": [
    "### Choosing a model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f9b35",
   "metadata": {},
   "source": [
    "Choosing the right model for your purposes is an important part of building chatbots! You can read on the different types of models available on the Hugging Face website: https://huggingface.co/models.\n",
    "\n",
    "LLMs differ from each other in how they are trained. Let's look at some examples to see how different models fit better in various contexts.\n",
    "\n",
    "* Text generation:\n",
    "If you need a general-purpose text generation model, consider using the GPT-2 or GPT-3 models. They are known for their impressive language generation capabilities.\n",
    "Example: You want to build a chatbot that generates creative and coherent responses to user input.\n",
    "\n",
    "* Sentiment analysis:\n",
    "For sentiment analysis tasks, models like BERT or RoBERTa are popular choices. They are trained to understand the sentiment and emotional tone of text.\n",
    "Example: You want to analyze customer feedback and determine whether it is positive or negative.\n",
    "\n",
    "* Named entity recognition:\n",
    "LLMs such as BERT, GPT-2, or RoBERTa can be used for Named Entity Recognition (NER) tasks. They perform well in understanding and extracting entities like person names, locations, organizations, etc.\n",
    "Example: You want to build a system that extracts names of people and places from a given text.\n",
    "\n",
    "* Question answering:\n",
    "Models like BERT, GPT-2, or XLNet can be effective for question-answering tasks. They can comprehend questions and provide accurate answers based on the given context.\n",
    "Example: You want to build a chatbot that can answer factual questions from a given set of documents.\n",
    "\n",
    "* Language translation:\n",
    "For language translation tasks, you can consider models like MarianMT or T5. They are designed specifically for translating text between different languages.\n",
    "Example: You want to build a language translation tool that translates English text to French.\n",
    "\n",
    "However, these examples are very limited and the fit of an LLM may depend on many factors such as data availability, performance requirements, resource constraints, and domain-specific considerations. It's important to explore different LLMs thoroughly and experiment with them to find the best match for your specific application.\n",
    "\n",
    "Other important purposes that should be taken into consideration when choosing an LLM include (but are not limited to):\n",
    "\n",
    "* Licensing: Ensure you are allowed to use your chosen model the way you intend\n",
    "* Model size: Larger models may be more accurate, but might also come at the cost of greater resource requirements\n",
    "* Training data: Ensure that the model's training data aligns with the domain or context you intend to use the LLM for\n",
    "* Performance and accuracy: Consider factors like accuracy, runtime, or any other metrics that are important for your specific use case\n",
    "\n",
    "To explore all the different options, check out the available models on the Hugging Face website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
